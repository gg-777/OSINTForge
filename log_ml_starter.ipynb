{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3376aec0",
   "metadata": {},
   "source": [
    "# Log Feature Engineering + LSTM Autoencoder (Starter)\n",
    "This starter notebook builds synthetic features and a tiny LSTM autoencoder to score anomalies. Replace with your real logs via OpenSearch queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "np.random.seed(42)\n",
    "hosts = ['host%03d' % i for i in range(50)]\n",
    "rows = []\n",
    "for h in hosts:\n",
    "    for t in range(1000):\n",
    "        rows.append({'host': h, 'hour': t,\n",
    "                     'failed_logins': np.random.poisson(0.2),\n",
    "                     'unique_dst_ips': np.random.poisson(1.2),\n",
    "                     'bytes': np.random.exponential(300),\n",
    "                     'process_spawn': np.random.poisson(0.6)})\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['failed_logins','unique_dst_ips','bytes','process_spawn']\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "h = 'host000'\n",
    "sub = df[df.host==h].sort_values('hour')\n",
    "seq_len = 20\n",
    "sequences = []\n",
    "for i in range(len(sub)-seq_len):\n",
    "    sequences.append(sub[features].iloc[i:i+seq_len].values)\n",
    "import numpy as np\n",
    "seqs = np.stack(sequences)\n",
    "print('seqs shape', seqs.shape)\n",
    "\n",
    "import torch\n",
    "X = torch.tensor(seqs, dtype=torch.float32)\n",
    "loader = DataLoader(TensorDataset(X), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_size=n_features, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(128, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 128)\n",
    "        self.decoder = nn.LSTM(input_size=128, hidden_size=n_features, num_layers=2, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out,_ = self.encoder(x)\n",
    "        h = out[:, -1, :]\n",
    "        z = self.fc(h)\n",
    "        dec_in = self.decoder_fc(z).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        out,_ = self.decoder(dec_in)\n",
    "        return out\n",
    "\n",
    "model = LSTMAutoencoder(n_features=len(features))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total=0\n",
    "    for (batch,) in loader:\n",
    "        recon = model(batch)\n",
    "        loss = loss_fn(recon, batch)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        total += loss.item()\n",
    "    print('epoch',epoch,'loss', total/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9eb041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    recon = model(X)\n",
    "    mse = torch.mean((recon - X)**2, dim=(1,2)).numpy()\n",
    "\n",
    "idx = np.argsort(mse)[-10:]\n",
    "mse[idx], idx[:10]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
